import torch
from torch.utils.data import Dataset as TorchDataset
import os, yaml, hashlib, json
from Data.DataUtils.DownloadUtils import InterProDownloader
from Data.DataUtils.Clustering import IdentitySearch, cdhit
from Data.DataUtils.Filtering import SeqFilter
from Data.DataUtils import Utils
import subprocess
import numpy as np

class Dataset(TorchDataset):

    def __init__(self, config, type, root='Data', config_root='Configs/Data'):

        assert type in ['Train', 'Val', 'Full'], "Unknown split type."

        root += '/'
        _ = [os.makedirs(root+subfolder, exist_ok=True) for subfolder in ['Raw', 'Filtered_preMSA', 'Filtered_postMSA', 'Clustered', 'Embedded', 'MSA']]
        self.root = root

        config_root += '/'
        self.config_root = config_root

        # load yml config file
        config = self.load_config(config)
        self.config = config

        # get hashed paths
        self.paths = self.compute_paths(config)

        # do all the steps
        self.download(config['download'])
        self.cluster(config['cluster'])
        self.filter_preMSA(config['filter_preMSA'])
        self.msa(config['msa'])
        self.filter_postMSA(config['filter_postMSA'])
        self.embed(config['embed'])

        # load data for training
        print('Loading data in.')

        if type == 'Full':
            self.data = self.read_tsv_data(self.paths['embed']+'.tsv')
            self.data_ids = self.read_tsv_ids(self.paths['embed']+'.tsv')
        else:
            full_data = self.read_tsv_data(self.paths['embed']+'.tsv')

            # shuffle, split into train and val, fixed seed of course
            np.random.seed(42)
            np.random.shuffle(full_data)
            train_proportion = config['split']['train_proportion']
            cutoff = int(train_proportion*len(full_data))
            train = full_data[:cutoff]
            val = full_data[cutoff:]

            # select if train or val
            if type == 'Train':
                self.data = train
            elif type == 'Val':
                self.data = val
            else:
                raise NotImplementedError


    def load_config(self, config):
        '''
        From TC
        Parse a config name, path, or object to a config object.
        '''
        # config is not a config object
        if type(config) == str:
            # config is a config name
            if not '/' in config and os.path.isfile(self.config_root+config+'.yml'):
                config = self.config_root+config+'.yml'
            # config is a config path
            if os.path.isfile(config):
                with open(config,'r') as file:
                    config = yaml.safe_load(file)
        return config

    def compute_paths(self, config):
        '''
        From TC
        Compute the hashes of the (sub-)Configs and return the paths for each processing step.
        Hashes are generated by including all previous steps.
        '''

        # compute the hash value of a dictionary-type config
        def hash(config):
            string = json.dumps(config, sort_keys=True)
            return hashlib.md5(string.encode('utf-8')).hexdigest()

        processing_steps = ['download', 'cluster', 'filter_preMSA', 'msa', 'filter_postMSA', 'embed']
        subfolders = ['Raw', 'Clustered', 'Filtered_preMSA', 'MSA', 'Filtered_postMSA', 'Embedded']

        # return one path with a hash value for every processing step
        return {
            key: self.root+subfolders[i]+'/'+hash([config[k] for k in processing_steps[:i+1]]) for i,key in enumerate(processing_steps)
        }

    def download(self, config):
        '''
        Download the sequence data from Interpro, given an Interpro ID.
        '''

        # check if previously done
        if os.path.exists(self.paths['download'] + '.fasta'):
            print('Downloading already done.')
            return

        # download raw data from interpro
        print('Downloading from InterPro. This might take a while...')

        # downloading fasta
        downloader = InterProDownloader()
        interpr_id = config['interpro_id']
        downloader.download_entry_proteins(interpr_id, self.paths['download'] + '.fasta')
        print('Raw data successfully downloaded.')

    def cluster(self, config):
        '''
        Cluster the sequences with method of choice, select one sequence per cluster
        '''

        # make necessary paths
        paths = [self.paths['cluster'] + '.fasta', self.paths['cluster'] + '.clstr']

        # check if previously done. If at least one file is missing redo.
        done = True
        for path in paths:
            if not os.path.exists(path):
                done = False
        if done:
            print('Clustering already done.')
            return

        print('Clustering...')

        # which clustering method should be used?
        threshold = config['threshold']
        if threshold == 1.0:
            clusterer = IdentitySearch()
        else:
            clusterer = cdhit()
            clusterer.c = threshold

        # clustering
        clusterer.cluster(self.paths['download'] + '.fasta', self.paths['cluster'])
        print('Clustering done.')


    def filter_preMSA(self, config):
        '''
        Filter the clustered data.
        '''

        # check if previously done
        if os.path.exists(self.paths['filter_preMSA'] + '.fasta'):
            print('Filtering already done.')
            return

        print('Filtering...')

        # read in filtered seqs
        seqs = Utils.read_fasta(self.paths['cluster'] + '.fasta')

        # filtering
        f = SeqFilter()
        seqs = f.filter_max_length(seqs, config['maximum_length'])
        seqs = f.filter_min_length(seqs, config['minimum_length'])
        if config['only_standard_amino_acids']:
            seqs = f.filter_standard_aas(seqs)

        # saving to file
        fasta_content = '\n'.join(['>' + head + '\n' + seq for head, seq in seqs.items()])
        open(self.paths['filter_preMSA'] + '.fasta', 'w').write(fasta_content)
        print('Filtering done.')


    def msa(self, config):
        '''
        run clustalo to make MSA.
        '''

        if os.path.exists(self.paths['msa'] + '.fasta'):
            print('MSA already done.')
            return

        in_path = self.paths['filter_preMSA'] + '.fasta'
        out_path = self.paths['msa'] + '.fasta'
        command = 'clustalo -i ' + in_path + ' -o ' + out_path + ' --outfmt fasta'

        if config['hmm'] != None:
            hmm_path = self.root + 'Raw/' + config['hmm']
            command += ' --hmm-in ' + hmm_path

        print('Running clustalo for MSA. This might take a while...')

        subprocess.call(command, shell=True)

        print('Done with MSA.')

    def filter_postMSA(self, config):
        '''
        Filter MSA for rare positions, delete empty columns.
        '''

        # check if previously done
        if os.path.exists(self.paths['filter_postMSA'] + '.fasta'):
            print('MSA filtering already done.')
            return

        print('Filtering MSA...')

        # read in filtered seqs
        seqs = Utils.read_fasta(self.paths['msa'] + '.fasta')

        # filtering
        f = SeqFilter()
        seqs = f.delete_seqs_with_rare_positions(seqs, config['column_frequency_cutoff'])
        seqs = f.delete_empty_msa_columns(seqs)

        # saving to file
        fasta_content = '\n'.join(['>' + head + '\n' + seq for head, seq in seqs.items()])
        open(self.paths['filter_postMSA'] + '.fasta', 'w').write(fasta_content)
        print('MSA filtering done.')


    def embed(self, config):
        '''
         Embed MSA to tsv file
         '''

        # check if previously done
        if os.path.exists(self.paths['embed'] + '.tsv'):
            print('Embedding already done.')
            return

        print('Embedding MSA...')

        # read in msa
        seqs = Utils.read_fasta(self.paths['filter_postMSA'] + '.fasta')

        tsv_content = ''

        # one hot encoding
        if config['sequence'] == 'onehot':
            tsv_content = '\n'.join([head + '\t' + ''.join([str(int(x)) for x in Utils.one_hot_encode(seq)]) for head, seq in seqs.items()])
        else:
            raise NotImplementedError

        # saving to file
        open(self.paths['embed'] + '.tsv', 'w').write(tsv_content)
        print('Embedding done.')

    def read_tsv_data(self, path):
        return [self.parse_encoding(x.split('\t')[1]) for x in open(path).read().strip().split('\n')]

    def read_tsv_ids(self, path):
        return [x.split('\t')[0] for x in open(path).read().strip().split('\n')]

    def parse_encoding(self, encoding):
        data = np.zeros((1, len(encoding)))
        for i, x in enumerate(encoding):
            data[0,i] = float(x)
        return torch.from_numpy(data).float()

    def delete_files(self):
        '''
        From TC
        Delete all files associated to this dataset.
        '''
        for path in self.paths.values():
            if os.path.isfile(path):
                os.remove(path)


    def __len__(self):
        return len(self.data)


    def __getitem__(self, idx):
        return self.data[idx]
